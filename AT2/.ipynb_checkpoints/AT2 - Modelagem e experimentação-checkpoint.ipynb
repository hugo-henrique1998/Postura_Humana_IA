{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ed7cba-fc06-4dd3-a039-e28cf538d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: torch in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchvision in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.7.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn]0m \u001b[32m3/4\u001b[0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa2a4f0-c96c-48ff-9201-2f38070f14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "\n",
    "# Função para limpar o campo 'image' no DataFrame\n",
    "def clean_img_name(name):\n",
    "    # Se for string no formato de lista (ex: \"['nome.jpg']\"), converte para lista e pega o primeiro item\n",
    "    if isinstance(name, str) and name.startswith('[') and name.endswith(']'):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(name)\n",
    "            if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                return parsed[0]\n",
    "        except:\n",
    "            pass\n",
    "    return name\n",
    "\n",
    "# Carregar CSV e limpar nomes das imagens\n",
    "df = pd.read_csv('../dados/dataset_pose.csv')\n",
    "df['image'] = df['image'].apply(clean_img_name)\n",
    "\n",
    "# Converter DataFrame em lista de dicionários\n",
    "samples = df.to_dict(orient='records')\n",
    "\n",
    "# Mapear ações para índices\n",
    "actions = sorted(set(s['action'] for s in samples))\n",
    "action_to_idx = {a: i for i, a in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb4337c-ec1c-4b49-87bf-49a53b3fda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn_pad(batch):\n",
    "    \"\"\"\n",
    "    batch: lista de tuplas (image, keypoints_tensor, label)\n",
    "    Faz padding dos keypoints para o tamanho do maior do batch.\n",
    "    \"\"\"\n",
    "    images = [item[0] for item in batch]\n",
    "    keypoints = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    \n",
    "    # Empilha imagens normalmente (mesmo shape)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Pega o tamanho máximo de keypoints no batch\n",
    "    max_kps = max(kp.shape[0] for kp in keypoints)\n",
    "    \n",
    "    # Padding dos keypoints com zeros\n",
    "    padded_kps = []\n",
    "    for kp in keypoints:\n",
    "        pad_size = max_kps - kp.shape[0]\n",
    "        if pad_size > 0:\n",
    "            padding = torch.zeros((pad_size, 3), dtype=torch.float32)\n",
    "            kp_padded = torch.cat([kp, padding], dim=0)\n",
    "        else:\n",
    "            kp_padded = kp\n",
    "        padded_kps.append(kp_padded)\n",
    "    \n",
    "    keypoints_batch = torch.stack(padded_kps)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return images, keypoints_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f7d442b-4294-41af-8770-29fd920d7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data, images_dir, img_key='image', keypoints_key='keypoints', label_key='action',\n",
    "                 image_size=(256, 256), transform=None):\n",
    "     \n",
    "        self.data = data\n",
    "        self.images_dir = images_dir\n",
    "        self.img_key = img_key\n",
    "        self.keypoints_key = keypoints_key\n",
    "        self.label_key = label_key\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Se não passar transform, cria padrão de redimensionar e converter em tensor\n",
    "        if self.transform is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(self.image_size),\n",
    "                T.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "    \n",
    "        img_filename = row[self.img_key]\n",
    "        if isinstance(img_filename, list):\n",
    "            img_filename = img_filename[0]\n",
    "        if isinstance(img_filename, str) and img_filename.startswith(\"[\") and img_filename.endswith(\"]\"):\n",
    "            img_filename = img_filename.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    \n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "    \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "    \n",
    "        keypoints = row[self.keypoints_key]\n",
    "    \n",
    "        if isinstance(keypoints, str):\n",
    "            keypoints = json.loads(keypoints)\n",
    "    \n",
    "        if isinstance(keypoints, list) and isinstance(keypoints[0], dict):\n",
    "            keypoints = [[kp['x'], kp['y']] for kp in keypoints]\n",
    "        keypoints = np.array(keypoints)\n",
    "        if keypoints.shape[1] == 2:\n",
    "            # Adiciona uma terceira coluna com 1 (visibilidade = 1, por exemplo)\n",
    "            vis = np.ones((keypoints.shape[0], 1))\n",
    "            keypoints = np.hstack((keypoints, vis))\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "    \n",
    "        label = row[self.label_key]\n",
    "        \n",
    "        # Se label for string numérica\n",
    "        try:\n",
    "            label = int(label)\n",
    "        except:\n",
    "            # se for categoria textual, faça seu mapeamento aqui\n",
    "            label_map = {'fishing and hunting': 0, 'occupation': 1, 'sports': 2}  # ajuste conforme seu caso\n",
    "            label = label_map[label]\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        max_kps = 16\n",
    "        if keypoints.shape[0] < max_kps:\n",
    "            pad_size = max_kps - keypoints.shape[0]\n",
    "            padding = torch.zeros(pad_size, keypoints.shape[1])  # assume (N,3)\n",
    "            keypoints = torch.cat([keypoints, padding], dim=0)\n",
    "        elif keypoints.shape[0] > max_kps:\n",
    "            keypoints = keypoints[:max_kps]\n",
    "    \n",
    "        return image, keypoints, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d817a53-df1c-4809-9112-aaaa38c53cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Ler CSV\n",
    "df = pd.read_csv('../dados/dataset_pose.csv')\n",
    "\n",
    "# 2. Criar mapeamento das ações para índices\n",
    "actions = sorted(df['action'].unique())\n",
    "action_to_idx = {a: i for i, a in enumerate(actions)}\n",
    "\n",
    "# 3. Mapear coluna 'action' para numérico\n",
    "df['action_idx'] = df['action'].map(action_to_idx)  # Usa a coluna esperada pela classe\n",
    "\n",
    "# 4. Separar treino/val (80/20), estratificado pela label\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['action_idx'], random_state=42)\n",
    "\n",
    "# 5. Converter DataFrame para lista de dicts (records)\n",
    "train_samples = train_df.to_dict('records')\n",
    "val_samples = val_df.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4dc0d056-023c-4ec4-9088-d79cb1493310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(train_samples, images_dir='../dados/mpii_human_pose_v1/images')\n",
    "val_dataset = PoseDataset(val_samples, images_dir='../dados/mpii_human_pose_v1/images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4302b554-e7fd-479c-8b26-3cd2deb7ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn_pad)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95d933-1101-41ab-a348-e6ebbde8b670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9308727-3bf0-4b85-9dd8-e25d09537600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce04b3f3-fa6f-4205-873f-58f17af10742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e2dafe8-87d9-4a63-b847-290cab7b449d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b030cc6-946f-405e-a1c4-9ca09e47d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PoseActionCNN(nn.Module):\n",
    "    def __init__(self, num_keypoints=16, num_classes=3):\n",
    "        super(PoseActionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  # (B, 32, 256, 256)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # (B, 32, 128, 128)\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),# (B, 64, 128, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # (B, 64, 64, 64)\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),# (B, 128, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # (B, 128, 32, 32)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 32 * 32, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_keypoints = nn.Linear(512, num_keypoints * 2)\n",
    "        self.fc_action = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        keypoints_out = self.fc_keypoints(x)\n",
    "        action_out = self.fc_action(x)\n",
    "\n",
    "        return keypoints_out, action_out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PoseActionCNN(num_keypoints=16, num_classes=3).to(device)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, mse_loss, ce_loss, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, keypoints, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)  # (B, 16*2)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred_kps, pred_action = model(images)\n",
    "        \n",
    "        loss_kps = mse_loss(pred_kps, keypoints)\n",
    "        loss_action = ce_loss(pred_action, labels)\n",
    "        loss = loss_kps + loss_action\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, mse_loss, ce_loss, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred_kps, pred_action = model(images)\n",
    "\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            loss_action = ce_loss(pred_action, labels)\n",
    "            loss = loss_kps + loss_action\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = pred_action.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    return total_loss / len(dataloader), acc\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, mse_loss, ce_loss, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, mse_loss, ce_loss, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f918892b-2714-4682-8603-baab57df25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Classe modelo adaptada para filtro variável (parâmetro)\n",
    "class PoseActionCNN(nn.Module):\n",
    "    def __init__(self, num_keypoints=16, num_classes=3, base_filters=32):\n",
    "        super(PoseActionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters, base_filters*2, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters*2, base_filters*4, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        # calcula o tamanho de entrada para fc de forma dinâmica (assumindo 256x256 input)\n",
    "        fc_input_size = base_filters*4 * 32 * 32\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_keypoints = nn.Linear(512, num_keypoints * 2)\n",
    "        self.fc_action = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        keypoints_out = self.fc_keypoints(x)\n",
    "        action_out = self.fc_action(x)\n",
    "        return keypoints_out, action_out\n",
    "\n",
    "# Funções train_one_epoch e validate continuam iguais (copie a sua)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, mse_loss, ce_loss, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, keypoints, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred_kps, pred_action = model(images)\n",
    "\n",
    "        loss_kps = mse_loss(pred_kps, keypoints)\n",
    "        loss_action = ce_loss(pred_action, labels)\n",
    "        loss = loss_kps + loss_action\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, mse_loss, ce_loss, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred_kps, pred_action = model(images)\n",
    "\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            loss_action = ce_loss(pred_action, labels)\n",
    "            loss = loss_kps + loss_action\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = pred_action.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    return total_loss / len(dataloader), acc\n",
    "\n",
    "# --- Validação cruzada com busca em hiperparâmetros ---\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def cross_validate(dataset, device, k=5, epochs=10, batch_size=32, param_grid=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = [\n",
    "            {'lr': 1e-3, 'base_filters': 32},\n",
    "            {'lr': 1e-3, 'base_filters': 64},\n",
    "            {'lr': 5e-4, 'base_filters': 32},\n",
    "        ]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(f\"\\nTestando parâmetros: lr={params['lr']}, base_filters={params['base_filters']}\")\n",
    "        fold_losses = []\n",
    "        fold_accuracies = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "            model = PoseActionCNN(num_keypoints=16, num_classes=3, base_filters=params['base_filters']).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            mse_loss = nn.MSELoss()\n",
    "            ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train_one_epoch(model, train_loader, optimizer, mse_loss, ce_loss, device)\n",
    "                val_loss, val_acc = validate(model, val_loader, mse_loss, ce_loss, device)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            fold_losses.append(val_loss)\n",
    "            fold_accuracies.append(val_acc)\n",
    "\n",
    "        avg_loss = np.mean(fold_losses)\n",
    "        avg_acc = np.mean(fold_accuracies)\n",
    "        print(f\"Parâmetros lr={params['lr']}, base_filters={params['base_filters']} | \"\n",
    "              f\"Média Val Loss: {avg_loss:.4f} | Média Val Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        results.append({'params': params, 'val_loss': avg_loss, 'val_acc': avg_acc})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867f05d-1afb-4aed-a8be-8004fafeb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando parâmetros: lr=0.001, base_filters=32\n",
      "Fold 1/5\n",
      "Epoch 1/10 - Val Loss: 116017.0820 | Val Acc: 0.3958\n",
      "Epoch 2/10 - Val Loss: 108775.4414 | Val Acc: 0.3958\n",
      "Epoch 3/10 - Val Loss: 103752.8438 | Val Acc: 0.3333\n",
      "Epoch 4/10 - Val Loss: 92876.3613 | Val Acc: 0.2708\n",
      "Epoch 5/10 - Val Loss: 86701.7480 | Val Acc: 0.3958\n",
      "Epoch 6/10 - Val Loss: 84826.5605 | Val Acc: 0.3958\n",
      "Epoch 7/10 - Val Loss: 88343.0664 | Val Acc: 0.3958\n",
      "Epoch 8/10 - Val Loss: 84034.5781 | Val Acc: 0.3333\n",
      "Epoch 9/10 - Val Loss: 87097.4961 | Val Acc: 0.3958\n",
      "Epoch 10/10 - Val Loss: 84223.8789 | Val Acc: 0.3333\n",
      "Fold 2/5\n",
      "Epoch 1/10 - Val Loss: 74108.2109 | Val Acc: 0.2708\n",
      "Epoch 2/10 - Val Loss: 82902.1289 | Val Acc: 0.3333\n",
      "Epoch 3/10 - Val Loss: 76706.7773 | Val Acc: 0.3958\n",
      "Epoch 4/10 - Val Loss: 61048.3770 | Val Acc: 0.2708\n",
      "Epoch 5/10 - Val Loss: 61149.6562 | Val Acc: 0.2708\n",
      "Epoch 6/10 - Val Loss: 56459.2402 | Val Acc: 0.3333\n",
      "Epoch 7/10 - Val Loss: 58045.3906 | Val Acc: 0.2917\n",
      "Epoch 8/10 - Val Loss: 54967.0059 | Val Acc: 0.3333\n",
      "Epoch 9/10 - Val Loss: 55386.4043 | Val Acc: 0.3333\n",
      "Epoch 10/10 - Val Loss: 55053.1738 | Val Acc: 0.3750\n",
      "Fold 3/5\n",
      "Epoch 1/10 - Val Loss: 120298.4922 | Val Acc: 0.3333\n",
      "Epoch 2/10 - Val Loss: 134306.6055 | Val Acc: 0.3958\n",
      "Epoch 3/10 - Val Loss: 121281.9609 | Val Acc: 0.3333\n",
      "Epoch 4/10 - Val Loss: 106992.1484 | Val Acc: 0.3958\n",
      "Epoch 5/10 - Val Loss: 103120.6602 | Val Acc: 0.3958\n",
      "Epoch 6/10 - Val Loss: 97925.0195 | Val Acc: 0.2708\n",
      "Epoch 7/10 - Val Loss: 97471.3398 | Val Acc: 0.3333\n",
      "Epoch 8/10 - Val Loss: 97134.6836 | Val Acc: 0.3333\n",
      "Epoch 9/10 - Val Loss: 95666.5078 | Val Acc: 0.2708\n"
     ]
    }
   ],
   "source": [
    "results = cross_validate(train_dataset, device=device, k=5, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf1a2c-2489-49c3-99d2-aa0f1310383d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefaacb5-c1d1-441b-9b7c-94b751457656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

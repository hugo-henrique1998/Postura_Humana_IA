{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ed7cba-fc06-4dd3-a039-e28cf538d7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install pandas\\n!pip install torch\\n!pip install torchvision\\n!pip install scikit-learn\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install pandas\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install scikit-learn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffe7dae-c152-4e23-bc13-23adf7095032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação de biblioteca\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Subset\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa2a4f0-c96c-48ff-9201-2f38070f14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para limpar o campo 'image' no DataFrame\n",
    "def clean_img_name(name):\n",
    "    # Se for string no formato de lista (ex: \"['nome.jpg']\"), converte para lista e pega o primeiro item\n",
    "    if isinstance(name, str) and name.startswith('[') and name.endswith(']'):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(name)\n",
    "            if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                return parsed[0]\n",
    "        except:\n",
    "            pass\n",
    "    return name\n",
    "\n",
    "# Carregar CSV e limpar nomes das imagens\n",
    "df = pd.read_csv('../dados/dataset_pose.csv')\n",
    "df['image'] = df['image'].apply(clean_img_name)\n",
    "\n",
    "# Converter DataFrame em lista de dicionários\n",
    "samples = df.to_dict(orient='records')\n",
    "\n",
    "# Mapear ações para índices\n",
    "actions = sorted(set(s['action'] for s in samples))\n",
    "action_to_idx = {a: i for i, a in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7d442b-4294-41af-8770-29fd920d7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe desenvolvida no ultimo projeto AT1\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data, images_dir, img_key='image', keypoints_key='keypoints', label_key='action',\n",
    "                 image_size=(256, 256), transform=None):\n",
    "     \n",
    "        self.data = data\n",
    "        self.images_dir = images_dir\n",
    "        self.img_key = img_key\n",
    "        self.keypoints_key = keypoints_key\n",
    "        self.label_key = label_key\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Se não passar transform, cria padrão de redimensionar e converter em tensor\n",
    "        if self.transform is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(self.image_size),\n",
    "                T.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "    \n",
    "        img_filename = row[self.img_key]\n",
    "        if isinstance(img_filename, list):\n",
    "            img_filename = img_filename[0]\n",
    "        if isinstance(img_filename, str) and img_filename.startswith(\"[\") and img_filename.endswith(\"]\"):\n",
    "            img_filename = img_filename.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    \n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "    \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "    \n",
    "        keypoints = row[self.keypoints_key]\n",
    "    \n",
    "        if isinstance(keypoints, str):\n",
    "            keypoints = json.loads(keypoints)\n",
    "    \n",
    "        if isinstance(keypoints, list) and isinstance(keypoints[0], dict):\n",
    "            keypoints = [[kp['x'], kp['y']] for kp in keypoints]\n",
    "        keypoints = np.array(keypoints)\n",
    "        if keypoints.shape[1] == 2:\n",
    "            # Adiciona uma terceira coluna com 1 (visibilidade = 1, por exemplo)\n",
    "            vis = np.ones((keypoints.shape[0], 1))\n",
    "            keypoints = np.hstack((keypoints, vis))\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "    \n",
    "        label = row[self.label_key]\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            label = int(label)\n",
    "        except:\n",
    "            \n",
    "            label_map = {'dancing': 0, 'miscellaneous': 1, 'sports': 2}  # ajuste conforme seu caso\n",
    "            label = label_map[label]\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        max_kps = 16\n",
    "        if keypoints.shape[0] < max_kps:\n",
    "            pad_size = max_kps - keypoints.shape[0]\n",
    "            padding = torch.zeros(pad_size, keypoints.shape[1])  # assume (N,3)\n",
    "            keypoints = torch.cat([keypoints, padding], dim=0)\n",
    "        elif keypoints.shape[0] > max_kps:\n",
    "            keypoints = keypoints[:max_kps]\n",
    "    \n",
    "        return image, keypoints, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb4337c-ec1c-4b49-87bf-49a53b3fda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para padronizar as amostras para gerar um batch\n",
    "def collate_fn_pad(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    keypoints = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    max_kps = max(kp.shape[0] for kp in keypoints)\n",
    "    \n",
    "  \n",
    "    padded_kps = []\n",
    "    for kp in keypoints:\n",
    "        pad_size = max_kps - kp.shape[0]\n",
    "        if pad_size > 0:\n",
    "            padding = torch.zeros((pad_size, 3), dtype=torch.float32)\n",
    "            kp_padded = torch.cat([kp, padding], dim=0)\n",
    "        else:\n",
    "            kp_padded = kp\n",
    "        padded_kps.append(kp_padded)\n",
    "    \n",
    "    keypoints_batch = torch.stack(padded_kps)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return images, keypoints_batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d817a53-df1c-4809-9112-aaaa38c53cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../dados/dataset_pose.csv')\n",
    "\n",
    "#Criar mapeamento das ações para índices\n",
    "actions = sorted(df['action'].unique())\n",
    "action_to_idx = {a: i for i, a in enumerate(actions)}\n",
    "\n",
    "\n",
    "df['action_idx'] = df['action'].map(action_to_idx)  # Usa a coluna esperada pela classe\n",
    "\n",
    "#Separar treino/val (80/20), estratificado pela label\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['action_idx'], random_state=42)\n",
    "\n",
    "train_samples = train_df.to_dict('records')\n",
    "val_samples = val_df.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc0d056-023c-4ec4-9088-d79cb1493310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(train_samples, images_dir='../dados/mpii_human_pose_v1/images')\n",
    "val_dataset = PoseDataset(val_samples, images_dir='../dados/mpii_human_pose_v1/images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4302b554-e7fd-479c-8b26-3cd2deb7ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn_pad)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35ff17c-e98d-496c-bf55-4bcdeced2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline de treinamento \n",
    "def evaluate_model(model, dataloader, device, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_kp_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            pred_kps, pred_action = model(images)\n",
    "\n",
    "            # Regressão: acumula MSE dos keypoints\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            total_kp_loss += loss_kps.item()\n",
    "\n",
    "            # Classificação: coleta predições e labels para classification_report\n",
    "            preds = pred_action.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_kp_loss = total_kp_loss / len(dataloader)\n",
    "    cls_report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(cls_report)\n",
    "    print(f\"Average Keypoints MSE Loss: {avg_kp_loss:.4f}\")\n",
    "\n",
    "    return cls_report, avg_kp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f918892b-2714-4682-8603-baab57df25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classe modelo adaptada para filtro variável (parâmetro)\n",
    "class PoseActionCNN(nn.Module):\n",
    "    def __init__(self, num_keypoints=16, num_classes=3, base_filters=32):\n",
    "        super(PoseActionCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters, base_filters*2, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters*2, base_filters*4, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((4, 4))  # reduz para (B, base_filters*4, 4, 4)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        fc_input_size = base_filters*4 * 4 * 4\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.fc_keypoints = nn.Linear(512, num_keypoints * 2)\n",
    "        self.fc_action = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return self.fc_keypoints(x), self.fc_action(x)\n",
    "\n",
    "# Funções train_one_epoch e validate continuam iguais (copie a sua)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, mse_loss, ce_loss, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, keypoints, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred_kps, pred_action = model(images)\n",
    "\n",
    "        loss_kps = mse_loss(pred_kps, keypoints)\n",
    "        loss_action = ce_loss(pred_action, labels)\n",
    "        loss = loss_kps + loss_action\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, mse_loss, ce_loss, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred_kps, pred_action = model(images)\n",
    "\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            loss_action = ce_loss(pred_action, labels)\n",
    "            loss = loss_kps + loss_action\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = pred_action.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    return total_loss / len(dataloader), acc\n",
    "\n",
    "# --- Validação cruzada com busca em hiperparâmetros ---\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def cross_validate(dataset, device, k=5, epochs=10, batch_size=32, param_grid=None):\n",
    "    from torch.utils.data import DataLoader, Subset\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    reports = []\n",
    "\n",
    "    # hiperparâmetros para grid search (exemplo)\n",
    "    if param_grid is None:\n",
    "        param_grid = [{'lr': 0.01, 'base_filters': 32}, {'lr': 0.1, 'base_filters': 64}]\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(f\"\\nTestando parâmetros: lr={params['lr']}, base_filters={params['base_filters']}\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            print(f\"\\nFold {fold+1}/{k}\")\n",
    "\n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "            model = PoseActionCNN(base_filters=params['base_filters']).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            mse_loss = nn.MSELoss()\n",
    "            ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train_one_epoch(model, train_loader, optimizer, mse_loss, ce_loss, device)\n",
    "                val_loss, val_acc = validate(model, val_loader, mse_loss, ce_loss, device)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Adiciona relatório detalhado após todas as épocas do fold\n",
    "            print(f\"\\nClassification Report for Fold {fold+1}:\")\n",
    "            report, kp_mse = evaluate_model(model, val_loader, device, class_names=['dancing', 'miscellaneous', 'sports'])\n",
    "            reports.append({'classification_report': report, 'keypoints_mse': kp_mse})\n",
    "\n",
    "            fold_results.append({'params': params, 'val_loss': val_loss, 'val_acc': val_acc})\n",
    "\n",
    "    return fold_results, reports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a867f05d-1afb-4aed-a8be-8004fafeb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando parâmetros: lr=0.01, base_filters=32\n",
      "\n",
      "Fold 1/4\n",
      "Epoch 1/5 - Val Loss: 370032.9062 | Val Acc: 0.3500\n",
      "Epoch 2/5 - Val Loss: 151869.7383 | Val Acc: 0.3667\n",
      "Epoch 3/5 - Val Loss: 89608.8854 | Val Acc: 0.3500\n",
      "Epoch 4/5 - Val Loss: 99925.7904 | Val Acc: 0.3667\n",
      "Epoch 5/5 - Val Loss: 87691.6068 | Val Acc: 0.3500\n",
      "\n",
      "Classification Report for Fold 1:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        51\n",
      "miscellaneous       0.35      1.00      0.52        63\n",
      "       sports       0.00      0.00      0.00        66\n",
      "\n",
      "     accuracy                           0.35       180\n",
      "    macro avg       0.12      0.33      0.17       180\n",
      " weighted avg       0.12      0.35      0.18       180\n",
      "\n",
      "Average Keypoints MSE Loss: 87686.5495\n",
      "\n",
      "Fold 2/4\n",
      "Epoch 1/5 - Val Loss: 94932.8802 | Val Acc: 0.3778\n",
      "Epoch 2/5 - Val Loss: 97043.3125 | Val Acc: 0.3611\n",
      "Epoch 3/5 - Val Loss: 76231.9596 | Val Acc: 0.2611\n",
      "Epoch 4/5 - Val Loss: 83805.3477 | Val Acc: 0.2611\n",
      "Epoch 5/5 - Val Loss: 89281.2591 | Val Acc: 0.3778\n",
      "\n",
      "Classification Report for Fold 2:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.38      1.00      0.55        68\n",
      "miscellaneous       0.00      0.00      0.00        47\n",
      "       sports       0.00      0.00      0.00        65\n",
      "\n",
      "     accuracy                           0.38       180\n",
      "    macro avg       0.13      0.33      0.18       180\n",
      " weighted avg       0.14      0.38      0.21       180\n",
      "\n",
      "Average Keypoints MSE Loss: 89272.2018\n",
      "\n",
      "Fold 3/4\n",
      "Epoch 1/5 - Val Loss: 317937.1250 | Val Acc: 0.3278\n",
      "Epoch 2/5 - Val Loss: 233381.1615 | Val Acc: 0.3611\n",
      "Epoch 3/5 - Val Loss: 80643.1758 | Val Acc: 0.3000\n",
      "Epoch 4/5 - Val Loss: 80514.8477 | Val Acc: 0.3000\n",
      "Epoch 5/5 - Val Loss: 77304.6725 | Val Acc: 0.3000\n",
      "\n",
      "Classification Report for Fold 3:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        67\n",
      "miscellaneous       0.00      0.00      0.00        59\n",
      "       sports       0.30      1.00      0.46        54\n",
      "\n",
      "     accuracy                           0.30       180\n",
      "    macro avg       0.10      0.33      0.15       180\n",
      " weighted avg       0.09      0.30      0.14       180\n",
      "\n",
      "Average Keypoints MSE Loss: 77302.6081\n",
      "\n",
      "Fold 4/4\n",
      "Epoch 1/5 - Val Loss: 356024.5182 | Val Acc: 0.3000\n",
      "Epoch 2/5 - Val Loss: 337174.0703 | Val Acc: 0.3944\n",
      "Epoch 3/5 - Val Loss: 99509.9466 | Val Acc: 0.3000\n",
      "Epoch 4/5 - Val Loss: 81313.0208 | Val Acc: 0.3056\n",
      "Epoch 5/5 - Val Loss: 81341.4837 | Val Acc: 0.3000\n",
      "\n",
      "Classification Report for Fold 4:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.30      1.00      0.46        54\n",
      "miscellaneous       0.00      0.00      0.00        71\n",
      "       sports       0.00      0.00      0.00        55\n",
      "\n",
      "     accuracy                           0.30       180\n",
      "    macro avg       0.10      0.33      0.15       180\n",
      " weighted avg       0.09      0.30      0.14       180\n",
      "\n",
      "Average Keypoints MSE Loss: 81338.8281\n",
      "\n",
      "Testando parâmetros: lr=0.1, base_filters=64\n",
      "\n",
      "Fold 1/4\n",
      "Epoch 1/5 - Val Loss: 97189.0091 | Val Acc: 0.3500\n",
      "Epoch 2/5 - Val Loss: 87824.1693 | Val Acc: 0.2833\n",
      "Epoch 3/5 - Val Loss: 94156.1953 | Val Acc: 0.3500\n",
      "Epoch 4/5 - Val Loss: 94784.3190 | Val Acc: 0.3667\n",
      "Epoch 5/5 - Val Loss: 87268.7878 | Val Acc: 0.2833\n",
      "\n",
      "Classification Report for Fold 1:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.28      1.00      0.44        51\n",
      "miscellaneous       0.00      0.00      0.00        63\n",
      "       sports       0.00      0.00      0.00        66\n",
      "\n",
      "     accuracy                           0.28       180\n",
      "    macro avg       0.09      0.33      0.15       180\n",
      " weighted avg       0.08      0.28      0.13       180\n",
      "\n",
      "Average Keypoints MSE Loss: 87260.5651\n",
      "\n",
      "Fold 2/4\n",
      "Epoch 1/5 - Val Loss: 346873.1406 | Val Acc: 0.2611\n",
      "Epoch 2/5 - Val Loss: 337246.3125 | Val Acc: 0.3611\n",
      "Epoch 3/5 - Val Loss: 307806.3281 | Val Acc: 0.3778\n",
      "Epoch 4/5 - Val Loss: 244155.6823 | Val Acc: 0.2611\n",
      "Epoch 5/5 - Val Loss: 163084.6914 | Val Acc: 0.3611\n",
      "\n",
      "Classification Report for Fold 2:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        68\n",
      "miscellaneous       0.00      0.00      0.00        47\n",
      "       sports       0.36      1.00      0.53        65\n",
      "\n",
      "     accuracy                           0.36       180\n",
      "    macro avg       0.12      0.33      0.18       180\n",
      " weighted avg       0.13      0.36      0.19       180\n",
      "\n",
      "Average Keypoints MSE Loss: 163082.6875\n",
      "\n",
      "Fold 3/4\n",
      "Epoch 1/5 - Val Loss: 180675.3411 | Val Acc: 0.3722\n",
      "Epoch 2/5 - Val Loss: 97594.5586 | Val Acc: 0.3000\n",
      "Epoch 3/5 - Val Loss: 79979.6654 | Val Acc: 0.3722\n",
      "Epoch 4/5 - Val Loss: 89104.1914 | Val Acc: 0.3722\n",
      "Epoch 5/5 - Val Loss: 77521.0866 | Val Acc: 0.3278\n",
      "\n",
      "Classification Report for Fold 3:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        67\n",
      "miscellaneous       0.33      1.00      0.49        59\n",
      "       sports       0.00      0.00      0.00        54\n",
      "\n",
      "     accuracy                           0.33       180\n",
      "    macro avg       0.11      0.33      0.16       180\n",
      " weighted avg       0.11      0.33      0.16       180\n",
      "\n",
      "Average Keypoints MSE Loss: 77515.0684\n",
      "\n",
      "Fold 4/4\n",
      "Epoch 1/5 - Val Loss: 355273.5391 | Val Acc: 0.2667\n",
      "Epoch 2/5 - Val Loss: 356883.8385 | Val Acc: 0.3944\n",
      "Epoch 3/5 - Val Loss: 356898.2969 | Val Acc: 0.3056\n",
      "Epoch 4/5 - Val Loss: 347700.8958 | Val Acc: 0.3556\n",
      "Epoch 5/5 - Val Loss: 339922.7161 | Val Acc: 0.3056\n",
      "\n",
      "Classification Report for Fold 4:\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        54\n",
      "miscellaneous       0.00      0.00      0.00        71\n",
      "       sports       0.31      1.00      0.47        55\n",
      "\n",
      "     accuracy                           0.31       180\n",
      "    macro avg       0.10      0.33      0.16       180\n",
      " weighted avg       0.09      0.31      0.14       180\n",
      "\n",
      "Average Keypoints MSE Loss: 339895.8359\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results, reports = cross_validate(train_dataset, device=device, k=4, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbf1a2c-2489-49c3-99d2-aa0f1310383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'lr': 0.01, 'base_filters': 32},\n",
       "  'val_loss': 87691.60677083333,\n",
       "  'val_acc': 0.35},\n",
       " {'params': {'lr': 0.01, 'base_filters': 32},\n",
       "  'val_loss': 89281.25911458333,\n",
       "  'val_acc': 0.37777777777777777},\n",
       " {'params': {'lr': 0.01, 'base_filters': 32},\n",
       "  'val_loss': 77304.67252604167,\n",
       "  'val_acc': 0.3},\n",
       " {'params': {'lr': 0.01, 'base_filters': 32},\n",
       "  'val_loss': 81341.48372395833,\n",
       "  'val_acc': 0.3},\n",
       " {'params': {'lr': 0.1, 'base_filters': 64},\n",
       "  'val_loss': 87268.78776041667,\n",
       "  'val_acc': 0.2833333333333333},\n",
       " {'params': {'lr': 0.1, 'base_filters': 64},\n",
       "  'val_loss': 163084.69140625,\n",
       "  'val_acc': 0.3611111111111111},\n",
       " {'params': {'lr': 0.1, 'base_filters': 64},\n",
       "  'val_loss': 77521.08658854167,\n",
       "  'val_acc': 0.3277777777777778},\n",
       " {'params': {'lr': 0.1, 'base_filters': 64},\n",
       "  'val_loss': 339922.7161458333,\n",
       "  'val_acc': 0.3055555555555556}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefaacb5-c1d1-441b-9b7c-94b751457656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.00      0.00      0.00        51\\nmiscellaneous       0.35      1.00      0.52        63\\n       sports       0.00      0.00      0.00        66\\n\\n     accuracy                           0.35       180\\n    macro avg       0.12      0.33      0.17       180\\n weighted avg       0.12      0.35      0.18       180\\n',\n",
       "  'keypoints_mse': 87686.54947916667},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.38      1.00      0.55        68\\nmiscellaneous       0.00      0.00      0.00        47\\n       sports       0.00      0.00      0.00        65\\n\\n     accuracy                           0.38       180\\n    macro avg       0.13      0.33      0.18       180\\n weighted avg       0.14      0.38      0.21       180\\n',\n",
       "  'keypoints_mse': 89272.20182291667},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.00      0.00      0.00        67\\nmiscellaneous       0.00      0.00      0.00        59\\n       sports       0.30      1.00      0.46        54\\n\\n     accuracy                           0.30       180\\n    macro avg       0.10      0.33      0.15       180\\n weighted avg       0.09      0.30      0.14       180\\n',\n",
       "  'keypoints_mse': 77302.60807291667},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.30      1.00      0.46        54\\nmiscellaneous       0.00      0.00      0.00        71\\n       sports       0.00      0.00      0.00        55\\n\\n     accuracy                           0.30       180\\n    macro avg       0.10      0.33      0.15       180\\n weighted avg       0.09      0.30      0.14       180\\n',\n",
       "  'keypoints_mse': 81338.828125},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.28      1.00      0.44        51\\nmiscellaneous       0.00      0.00      0.00        63\\n       sports       0.00      0.00      0.00        66\\n\\n     accuracy                           0.28       180\\n    macro avg       0.09      0.33      0.15       180\\n weighted avg       0.08      0.28      0.13       180\\n',\n",
       "  'keypoints_mse': 87260.56510416667},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.00      0.00      0.00        68\\nmiscellaneous       0.00      0.00      0.00        47\\n       sports       0.36      1.00      0.53        65\\n\\n     accuracy                           0.36       180\\n    macro avg       0.12      0.33      0.18       180\\n weighted avg       0.13      0.36      0.19       180\\n',\n",
       "  'keypoints_mse': 163082.6875},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.00      0.00      0.00        67\\nmiscellaneous       0.33      1.00      0.49        59\\n       sports       0.00      0.00      0.00        54\\n\\n     accuracy                           0.33       180\\n    macro avg       0.11      0.33      0.16       180\\n weighted avg       0.11      0.33      0.16       180\\n',\n",
       "  'keypoints_mse': 77515.068359375},\n",
       " {'classification_report': '               precision    recall  f1-score   support\\n\\n      dancing       0.00      0.00      0.00        54\\nmiscellaneous       0.00      0.00      0.00        71\\n       sports       0.31      1.00      0.47        55\\n\\n     accuracy                           0.31       180\\n    macro avg       0.10      0.33      0.16       180\\n weighted avg       0.09      0.31      0.14       180\\n',\n",
       "  'keypoints_mse': 339895.8359375}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9dd685-9d3e-4bfb-8330-93210f2ad384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo YOLO inspirado simples\n",
    "class YOLOInspiredNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=16, num_classes=3, base_filters=32):\n",
    "        super(YOLOInspiredNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, base_filters//2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters//2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters//2, base_filters, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters, base_filters*2, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters*2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(base_filters*2, base_filters*4, 3, padding=1),\n",
    "            nn.BatchNorm2d(base_filters*4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.AdaptiveAvgPool2d((4,4))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_filters*4*4*4, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc_keypoints = nn.Linear(512, num_keypoints*2)\n",
    "        self.fc_class = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return self.fc_keypoints(x), self.fc_class(x)\n",
    "\n",
    "# Treinamento por época\n",
    "def train_one_epoch(model, dataloader, optimizer, mse_loss, ce_loss, device, loss_weights=(1.0,1.0)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, keypoints, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        pred_kps, pred_cls = model(images)\n",
    "\n",
    "        loss_kps = mse_loss(pred_kps, keypoints)\n",
    "        loss_cls = ce_loss(pred_cls, labels)\n",
    "        loss = loss_weights[0]*loss_kps + loss_weights[1]*loss_cls\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validação simples (retorna loss e acurácia)\n",
    "def validate(model, dataloader, mse_loss, ce_loss, device, loss_weights=(1.0,1.0)):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            pred_kps, pred_cls = model(images)\n",
    "\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            loss_cls = ce_loss(pred_cls, labels)\n",
    "            loss = loss_weights[0]*loss_kps + loss_weights[1]*loss_cls\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = pred_cls.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    return total_loss / len(dataloader), acc\n",
    "\n",
    "# Validação detalhada com relatório de métricas\n",
    "def validate_with_metrics(model, dataloader, mse_loss, ce_loss, device, loss_weights=(1.0,1.0), class_names=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, keypoints, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            keypoints = keypoints[:, :, :2].reshape(images.size(0), -1).to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            pred_kps, pred_cls = model(images)\n",
    "\n",
    "            loss_kps = mse_loss(pred_kps, keypoints)\n",
    "            loss_cls = ce_loss(pred_cls, labels)\n",
    "            loss = loss_weights[0]*loss_kps + loss_weights[1]*loss_cls\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = pred_cls.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    return avg_loss, report\n",
    "\n",
    "def classification_report_from_dict(report_dict):\n",
    "    lines = []\n",
    "    for label, metrics in report_dict.items():\n",
    "        if label in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            lines.append(f\"{label:>12} : {metrics if isinstance(metrics, float) else metrics['f1-score']:.4f}\")\n",
    "        else:\n",
    "            lines.append(f\"{label:>12} : precision: {metrics['precision']:.4f} | recall: {metrics['recall']:.4f} | f1-score: {metrics['f1-score']:.4f} | support: {metrics['support']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Cross-Validation com grid search e nomes das classes\n",
    "def cross_validate(dataset, device,\n",
    "                   k=5, epochs=10,\n",
    "                   param_grid=None,\n",
    "                   num_keypoints=16,\n",
    "                   num_classes=3,\n",
    "                   loss_weights=(1.0, 1.0),\n",
    "                   class_names=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = [\n",
    "            {'lr': 0.001, 'base_filters': 32, 'batch_size': 32},\n",
    "            {'lr': 0.0001, 'base_filters': 64, 'batch_size': 64}\n",
    "        ]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(f\"\\nTestando parâmetros: lr={params['lr']}, base_filters={params['base_filters']}, batch_size={params['batch_size']}\")\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            print(f\"\\nFold {fold+1}/{k}\")\n",
    "\n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True, num_workers=2)\n",
    "            val_loader = DataLoader(val_subset, batch_size=params['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "            model = YOLOInspiredNet(num_keypoints=num_keypoints, num_classes=num_classes, base_filters=params['base_filters']).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            mse_loss = nn.MSELoss()\n",
    "            ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train_one_epoch(model, train_loader, optimizer, mse_loss, ce_loss, device, loss_weights)\n",
    "                val_loss, val_acc = validate(model, val_loader, mse_loss, ce_loss, device, loss_weights)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            val_loss, class_report = validate_with_metrics(model, val_loader, mse_loss, ce_loss, device, loss_weights, class_names)\n",
    "            print(f\"\\nClassification Report Fold {fold+1}:\")\n",
    "            print(classification_report_from_dict(class_report))\n",
    "\n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'fold': fold + 1,\n",
    "                'val_loss': val_loss,\n",
    "                'classification_report': class_report\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62901975-6c71-4ceb-bcb3-603eec539182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando parâmetros: lr=0.001, base_filters=32, batch_size=32\n",
      "\n",
      "Fold 1/4\n",
      "Epoch 1/5 - Train Loss: 320461.3860 | Val Loss: 339051.6276 | Val Acc: 0.3500\n",
      "Epoch 2/5 - Train Loss: 199842.3539 | Val Loss: 181134.7969 | Val Acc: 0.4389\n",
      "Epoch 3/5 - Train Loss: 93741.7838 | Val Loss: 144595.5924 | Val Acc: 0.3667\n",
      "Epoch 4/5 - Train Loss: 79936.6758 | Val Loss: 90720.3620 | Val Acc: 0.3500\n",
      "Epoch 5/5 - Train Loss: 76308.7652 | Val Loss: 86824.7982 | Val Acc: 0.3667\n",
      "\n",
      "Classification Report Fold 1:\n",
      "     dancing : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 51.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 63.0\n",
      "      sports : precision: 0.3667 | recall: 1.0000 | f1-score: 0.5366 | support: 66.0\n",
      "    accuracy : 0.3667\n",
      "   macro avg : 0.1789\n",
      "weighted avg : 0.1967\n",
      "\n",
      "Fold 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 328384.5607 | Val Loss: 291359.0885 | Val Acc: 0.3833\n",
      "Epoch 2/5 - Train Loss: 202833.2178 | Val Loss: 126890.7240 | Val Acc: 0.3778\n",
      "Epoch 3/5 - Train Loss: 95280.3031 | Val Loss: 80539.9518 | Val Acc: 0.3611\n",
      "Epoch 4/5 - Train Loss: 81802.0728 | Val Loss: 77119.3945 | Val Acc: 0.3611\n",
      "Epoch 5/5 - Train Loss: 80795.4511 | Val Loss: 83907.8587 | Val Acc: 0.3778\n",
      "\n",
      "Classification Report Fold 2:\n",
      "     dancing : precision: 0.3778 | recall: 1.0000 | f1-score: 0.5484 | support: 68.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 47.0\n",
      "      sports : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 65.0\n",
      "    accuracy : 0.3778\n",
      "   macro avg : 0.1828\n",
      "weighted avg : 0.2072\n",
      "\n",
      "Fold 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 339153.0680 | Val Loss: 269149.6771 | Val Acc: 0.3000\n",
      "Epoch 2/5 - Train Loss: 206698.3267 | Val Loss: 127667.4258 | Val Acc: 0.3000\n",
      "Epoch 3/5 - Train Loss: 92822.8171 | Val Loss: 82518.7454 | Val Acc: 0.3278\n",
      "Epoch 4/5 - Train Loss: 81003.6742 | Val Loss: 101708.0924 | Val Acc: 0.3722\n",
      "Epoch 5/5 - Train Loss: 78308.4527 | Val Loss: 78406.8685 | Val Acc: 0.3722\n",
      "\n",
      "Classification Report Fold 3:\n",
      "     dancing : precision: 0.3722 | recall: 1.0000 | f1-score: 0.5425 | support: 67.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 59.0\n",
      "      sports : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 54.0\n",
      "    accuracy : 0.3722\n",
      "   macro avg : 0.1808\n",
      "weighted avg : 0.2019\n",
      "\n",
      "Fold 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 326351.7868 | Val Loss: 320881.2969 | Val Acc: 0.3000\n",
      "Epoch 2/5 - Train Loss: 200153.9784 | Val Loss: 181242.3086 | Val Acc: 0.3000\n",
      "Epoch 3/5 - Train Loss: 92145.8807 | Val Loss: 118123.7142 | Val Acc: 0.3944\n",
      "Epoch 4/5 - Train Loss: 79555.7144 | Val Loss: 81582.1654 | Val Acc: 0.3000\n",
      "Epoch 5/5 - Train Loss: 77940.9543 | Val Loss: 84114.3841 | Val Acc: 0.3056\n",
      "\n",
      "Classification Report Fold 4:\n",
      "     dancing : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 54.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 71.0\n",
      "      sports : precision: 0.3056 | recall: 1.0000 | f1-score: 0.4681 | support: 55.0\n",
      "    accuracy : 0.3056\n",
      "   macro avg : 0.1560\n",
      "weighted avg : 0.1430\n",
      "\n",
      "Testando parâmetros: lr=0.0001, base_filters=64, batch_size=64\n",
      "\n",
      "Fold 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 339673.0000 | Val Loss: 383709.9271 | Val Acc: 0.3500\n",
      "Epoch 2/5 - Train Loss: 338058.5764 | Val Loss: 382311.3125 | Val Acc: 0.2833\n",
      "Epoch 3/5 - Train Loss: 335783.2778 | Val Loss: 379583.0833 | Val Acc: 0.2667\n",
      "Epoch 4/5 - Train Loss: 332761.4861 | Val Loss: 373605.3125 | Val Acc: 0.3500\n",
      "Epoch 5/5 - Train Loss: 323742.9688 | Val Loss: 363411.9062 | Val Acc: 0.2778\n",
      "\n",
      "Classification Report Fold 1:\n",
      "     dancing : precision: 0.2793 | recall: 0.9804 | f1-score: 0.4348 | support: 51.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 63.0\n",
      "      sports : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 66.0\n",
      "    accuracy : 0.2778\n",
      "   macro avg : 0.1449\n",
      "weighted avg : 0.1232\n",
      "\n",
      "Fold 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 355951.4896 | Val Loss: 348117.2396 | Val Acc: 0.3778\n",
      "Epoch 2/5 - Train Loss: 346215.1076 | Val Loss: 346773.6562 | Val Acc: 0.3722\n",
      "Epoch 3/5 - Train Loss: 345961.9236 | Val Loss: 343732.6146 | Val Acc: 0.2889\n",
      "Epoch 4/5 - Train Loss: 342547.1910 | Val Loss: 338388.1667 | Val Acc: 0.3667\n",
      "Epoch 5/5 - Train Loss: 332358.1910 | Val Loss: 330513.2708 | Val Acc: 0.3778\n",
      "\n",
      "Classification Report Fold 2:\n",
      "     dancing : precision: 0.3689 | recall: 0.5588 | f1-score: 0.4444 | support: 68.0\n",
      "miscellaneous : precision: 0.3208 | recall: 0.3617 | f1-score: 0.3400 | support: 47.0\n",
      "      sports : precision: 0.5417 | recall: 0.2000 | f1-score: 0.2921 | support: 65.0\n",
      "    accuracy : 0.3778\n",
      "   macro avg : 0.3589\n",
      "weighted avg : 0.3622\n",
      "\n",
      "Fold 3/4\n",
      "Epoch 1/5 - Train Loss: 355829.5139 | Val Loss: 318292.3333 | Val Acc: 0.3722\n",
      "Epoch 2/5 - Train Loss: 361980.8542 | Val Loss: 316854.6146 | Val Acc: 0.3722\n",
      "Epoch 3/5 - Train Loss: 360582.7917 | Val Loss: 313574.9479 | Val Acc: 0.3278\n",
      "Epoch 4/5 - Train Loss: 357614.7951 | Val Loss: 307175.3229 | Val Acc: 0.3167\n",
      "Epoch 5/5 - Train Loss: 345394.1632 | Val Loss: 298969.2708 | Val Acc: 0.3389\n",
      "\n",
      "Classification Report Fold 3:\n",
      "     dancing : precision: 0.3710 | recall: 0.3433 | f1-score: 0.3566 | support: 67.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 59.0\n",
      "      sports : precision: 0.3220 | recall: 0.7037 | f1-score: 0.4419 | support: 54.0\n",
      "    accuracy : 0.3389\n",
      "   macro avg : 0.2661\n",
      "weighted avg : 0.2653\n",
      "\n",
      "Fold 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 349648.4722 | Val Loss: 357069.0000 | Val Acc: 0.3167\n",
      "Epoch 2/5 - Train Loss: 347207.1493 | Val Loss: 355487.1042 | Val Acc: 0.3056\n",
      "Epoch 3/5 - Train Loss: 342407.9479 | Val Loss: 352775.4062 | Val Acc: 0.3389\n",
      "Epoch 4/5 - Train Loss: 337912.0799 | Val Loss: 346795.2500 | Val Acc: 0.3778\n",
      "Epoch 5/5 - Train Loss: 332932.8819 | Val Loss: 337216.3854 | Val Acc: 0.3000\n",
      "\n",
      "Classification Report Fold 4:\n",
      "     dancing : precision: 0.3000 | recall: 1.0000 | f1-score: 0.4615 | support: 54.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 71.0\n",
      "      sports : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 55.0\n",
      "    accuracy : 0.3000\n",
      "   macro avg : 0.1538\n",
      "weighted avg : 0.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hugo-henrique/miniconda3/envs/CaseIaAT2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "class_names = ['dancing', 'miscellaneous', 'sports']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results2 = cross_validate(train_dataset, device, k=4, epochs=5,class_names=class_names)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "649bcd64-121d-4bc8-a5f5-a21878c92183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_macro_f1(report_str):\n",
    "    for line in report_str.split('\\n'):\n",
    "        if line.strip().startswith('macro avg'):\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            if len(parts) >= 5:\n",
    "                return float(parts[3])\n",
    "    return 0.0\n",
    "\n",
    "def extract_accuracy(report_str):\n",
    "    for line in report_str.split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            for part in parts:\n",
    "                try:\n",
    "                    return float(part)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4e26eb-94f6-42c0-b2d4-ec99c5754904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor Fold: 1\n",
      "Modelo: N/A\n",
      "Parâmetros: {'lr': 0.01, 'base_filters': 32}\n",
      "F1-score macro: 0.3300\n",
      "Acurácia: 0.3500\n",
      "Keypoints MSE: 87686.55\n",
      "\n",
      "Relatório de classificação completo:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      dancing       0.00      0.00      0.00        51\n",
      "miscellaneous       0.35      1.00      0.52        63\n",
      "       sports       0.00      0.00      0.00        66\n",
      "\n",
      "     accuracy                           0.35       180\n",
      "    macro avg       0.12      0.33      0.17       180\n",
      " weighted avg       0.12      0.35      0.18       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_idx = max(range(len(reports)), key=lambda i: extract_macro_f1(reports[i]['classification_report']))\n",
    "\n",
    "print(f\"Melhor Fold: {best_idx + 1}\")\n",
    "print(f\"Modelo: {results[best_idx].get('model_name', 'N/A')}\")\n",
    "print(f\"Parâmetros: {results[best_idx].get('params', {})}\")\n",
    "print(f\"F1-score macro: {extract_macro_f1(reports[best_idx]['classification_report']):.4f}\")\n",
    "print(f\"Acurácia: {extract_accuracy(reports[best_idx]['classification_report']):.4f}\")\n",
    "print(f\"Keypoints MSE: {reports[best_idx]['keypoints_mse']:.2f}\")\n",
    "print(\"\\nRelatório de classificação completo:\\n\")\n",
    "print(reports[best_idx]['classification_report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06248808-4dad-4e6b-8ff4-a792e9dc8926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor resultado:\n",
      "Parâmetros: {'lr': 0.001, 'base_filters': 32, 'batch_size': 32}\n",
      "Fold: 3\n",
      "Val Loss: 78406.8685\n",
      "F1 macro: 0.1808\n",
      "\n",
      "Classification Report completo:\n",
      "     dancing : precision: 0.3722 | recall: 1.0000 | f1-score: 0.5425 | support: 67.0\n",
      "miscellaneous : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 59.0\n",
      "      sports : precision: 0.0000 | recall: 0.0000 | f1-score: 0.0000 | support: 54.0\n",
      "    accuracy : 0.3722\n",
      "   macro avg : 0.1808\n",
      "weighted avg : 0.2019\n"
     ]
    }
   ],
   "source": [
    "def best_result(results):\n",
    "    best_idx = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for i, res in enumerate(results):\n",
    "        val_loss = res['val_loss']\n",
    "        f1_macro = res['classification_report']['macro avg']['f1-score']\n",
    "        # Exemplo de score combinando F1 (positivo) e val_loss (negativo)\n",
    "        score = f1_macro - val_loss  # Você pode ajustar peso aqui\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "\n",
    "    return results[best_idx]\n",
    "\n",
    "\n",
    "# Exemplo de uso após cross_validate:\n",
    "\n",
    "melhor = best_result(results2)\n",
    "\n",
    "print(\"Melhor resultado:\")\n",
    "print(f\"Parâmetros: {melhor['params']}\")\n",
    "print(f\"Fold: {melhor['fold']}\")\n",
    "print(f\"Val Loss: {melhor['val_loss']:.4f}\")\n",
    "print(f\"F1 macro: {melhor['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(\"\\nClassification Report completo:\")\n",
    "print(classification_report_from_dict(melhor['classification_report']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdc959-076b-4b12-ac1f-23498c2ca2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
